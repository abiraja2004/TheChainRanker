{
 "metadata": {
  "name": "",
  "signature": "sha256:201c158b02369846176b9dbf54878cee2dbdee86fd448fd89ce8ee69c6cbb89c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk.corpus import wordnet_ic\n",
      " \n",
      "import sys\n",
      "reload(sys)\n",
      "sys.setdefaultencoding('utf8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "threshold = 0.6 #treshold for wup\n",
      "jcnTreshold = 0.09 #jcn\n",
      "pathTeshold = 0.1 #path\n",
      "brown_ic = wordnet_ic.ic('ic-brown.dat') #load the brown corpus\n",
      "lexical_chains = [] #empty list to hold all the chains\n",
      "dictionary = {} #empty dictionart to hold the count of each word encountered"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#class Chain \n",
      "class Chain(): \n",
      "    def __init__(self, words, senses, count = 0):\n",
      "\t\tself.words = set(words)\n",
      "\t\tself.senses = set(senses)\n",
      "\t\tself.score = 0\n",
      "\t\tdictionary[words[0]] = 1 #initialize counter\n",
      "\t\n",
      "    def addWord(self, word):\n",
      "        \n",
      "        if(len(self.words.intersection([word])) > 0):\n",
      "            dictionary[word] += 1\n",
      "        else:\n",
      "            dictionary[word] = 1\n",
      "        \n",
      "        self.words.add(word)\n",
      "\t\n",
      "\n",
      "    def addSense(self, sense):\n",
      "\t\tself.senses.add(sense)\n",
      "\n",
      "    def getWords(self):\n",
      "\t\treturn self.words\n",
      "\n",
      "    def getSenses(self):\n",
      "\t\treturn self.getSenses\n",
      "\n",
      "    def incCount(self):\n",
      "        self.count += 1\n",
      "\n",
      "    def setScore(self, sc):\n",
      "\t\tself.score = sc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_word(word):\n",
      "    maximum = 0 \n",
      "    maxJCN = 0\n",
      "    flag = 0\n",
      "    for chain in lexical_chains: #for all chains that are present\n",
      "\tfor synset in wn.synsets(word): #for all synsets of current word\n",
      "\t    for sense in chain.senses:  #for all senses of the current word in current element of the current chain\n",
      "\t        similarity = sense.wup_similarity(synset) #using wup_similarity\n",
      "\t        \n",
      "\t        if(similarity >= maximum):\n",
      "\t            if similarity >= threshold:\n",
      "\t                #print word, synset, sense, sense.jcn_similarity(synset, brown_ic)\n",
      "\t                JCN = sense.jcn_similarity(synset, brown_ic) #using jcn_similarity\n",
      "\t                if JCN >= jcnTreshold: \n",
      "\t                    if sense.path_similarity(synset) >= 0.2: #using path similarity\n",
      "\t                        if JCN >= maxJCN:\n",
      "\t                            maximum = similarity\n",
      "\t                            maxJCN = JCN\n",
      "\t                            maxChain = chain\n",
      "\t                            flag = 1\n",
      "    if flag == 1:\t               \t                    \n",
      "        maxChain.addWord(word)\n",
      "        maxChain.addSense(synset)\n",
      "        return\n",
      "\t\t    \n",
      "    lexical_chains.append(Chain([word], wn.synsets(word)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fileName = raw_input(\"Enter file path + name, if file name is 'nlp.txt', type 'nlp' \\n \\n\")\n",
      "fileName += \".txt\"\n",
      "print (\"\\n\\n\")\n",
      "#fileName = \"nlp.txt\"\n",
      "File = open(fileName) #open file\n",
      "lines = File.read() #read all lines\n",
      "#dec_lines =  [line.decode('utf-8') for line in lines] \n",
      "\n",
      "is_noun = lambda x: True if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS') else False\n",
      "nouns = [word for (word, pos) in nltk.pos_tag(nltk.word_tokenize(lines)) if is_noun(pos)]  #extract all nouns\n",
      "\n",
      "for word in nouns:\n",
      "    add_word(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#print all chains\n",
      "for chain in lexical_chains:\n",
      "\tchain_length = 0\n",
      "\tdis_word = 0\n",
      "\tfor word in chain.getWords():\n",
      "\t\t#print str(word + \"(\" + str(dictionary[word]) + \")\") + ',',\n",
      "\t\tchain_length = chain_length + dictionary[word]\n",
      "\t\tdis_word = dis_word + 1\n",
      "\t#print 'Length =' + str(chain_length)\n",
      "\thom = 1 - (dis_word*1.0/chain_length)\n",
      "\t#print 'Homogeneity =' + str(hom)\n",
      "\tscore = 1.0*chain_length*hom\n",
      "\t#print 'Score =' + str(score)\n",
      "\tchain.setScore(score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Sorted start'\n",
      "lexical_chains.sort(key=lambda x: x.score, reverse=True)\n",
      "\n",
      "for chain in lexical_chains:\n",
      "\tif(chain.score>0.0):\n",
      "\t\tfor word in chain.getWords():\n",
      "\t\t\tprint str(word + \"(\" + str(dictionary[word]) + \")\") + ',',\n",
      "\t\tprint 'Score=' + str(chain.score)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}